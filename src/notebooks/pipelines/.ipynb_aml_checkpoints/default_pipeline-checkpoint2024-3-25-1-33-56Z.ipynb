{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "# authenticate\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "SUBSCRIPTION=\"a3f56f48-3efb-4970-81a3-e4eda598333c\"\n",
        "RESOURCE_GROUP=\"luiz.victor.dev-rg\"\n",
        "WS_NAME=\"tcc-experiments\"\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=SUBSCRIPTION,\n",
        "    resource_group_name=RESOURCE_GROUP,\n",
        "    workspace_name=WS_NAME,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1714008757766
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that the handle works correctly.  \n",
        "# If you ge an error here, modify your SUBSCRIPTION, RESOURCE_GROUP, and WS_NAME in the previous cell.\n",
        "ws = ml_client.workspaces.get(WS_NAME)\n",
        "print(ws.location,\":\", ws.resource_group)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "eastus2 : luiz.victor.dev-rg\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1714008758506
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DATAS = [\n",
        "    \"vrex_1999_2000_2001_2002_2003_.csv\",\n",
        "    \"vrex_2004_2005_2006_2007_2008_.csv\",\n",
        "    \"vrex_2009_2010_2011_2012_2013_.csv\",\n",
        "    \"vrex_2014_2015_2016_2017_2018_.csv\",\n",
        "]\n",
        "\n",
        "TEST_DATAS = [\n",
        "    \"vrex_2004.csv\",\n",
        "    \"vrex_2009.csv\",\n",
        "    \"vrex_2014.csv\",\n",
        "    \"vrex_2019.csv\"\n",
        "]\n",
        "\n",
        "version = \"original\"\n",
        "\n",
        "arr_data_to_train = []\n",
        "arr_data_to_test = []\n",
        "\n",
        "for to_train, to_test in zip(TRAIN_DATAS, TEST_DATAS):\n",
        "    data_to_train = ml_client.data.get(name=to_train.split(\".\")[0], version=version)\n",
        "    arr_data_to_train.append(data_to_train)\n",
        "    print(f\"Data to train asset URI: {data_to_train.path} - name: {to_train.split('.')[0]}\")\n",
        "\n",
        "    data_to_test = ml_client.data.get(name=to_test.split('.')[0], version=version)\n",
        "    arr_data_to_test.append(data_to_test)\n",
        "    print(f\"Data to test asset URI: {data_to_test.path} - name: {to_test.split('.')[0]}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Data to train asset URI: azureml://subscriptions/a3f56f48-3efb-4970-81a3-e4eda598333c/resourcegroups/luiz.victor.dev-rg/workspaces/tcc-experiments/datastores/workspaceblobstore/paths/LocalUpload/bb48172e3ccf65075606876bcf5df9f2/vrex_1999_2000_2001_2002_2003_.csv - name: vrex_1999_2000_2001_2002_2003_\nData to test asset URI: azureml://subscriptions/a3f56f48-3efb-4970-81a3-e4eda598333c/resourcegroups/luiz.victor.dev-rg/workspaces/tcc-experiments/datastores/workspaceblobstore/paths/LocalUpload/3b205fc48a24468cd8575de6f0de7ae0/vrex_2004.csv - name: vrex_2004\nData to train asset URI: azureml://subscriptions/a3f56f48-3efb-4970-81a3-e4eda598333c/resourcegroups/luiz.victor.dev-rg/workspaces/tcc-experiments/datastores/workspaceblobstore/paths/LocalUpload/992a66ec6cfe9db64c4d0613ce5b5208/vrex_2004_2005_2006_2007_2008_.csv - name: vrex_2004_2005_2006_2007_2008_\nData to test asset URI: azureml://subscriptions/a3f56f48-3efb-4970-81a3-e4eda598333c/resourcegroups/luiz.victor.dev-rg/workspaces/tcc-experiments/datastores/workspaceblobstore/paths/LocalUpload/79fba3ceef6debdaff3901ea3d103366/vrex_2009.csv - name: vrex_2009\nData to train asset URI: azureml://subscriptions/a3f56f48-3efb-4970-81a3-e4eda598333c/resourcegroups/luiz.victor.dev-rg/workspaces/tcc-experiments/datastores/workspaceblobstore/paths/LocalUpload/20c8eaed3e0076d5623ceabb00067339/vrex_2009_2010_2011_2012_2013_.csv - name: vrex_2009_2010_2011_2012_2013_\nData to test asset URI: azureml://subscriptions/a3f56f48-3efb-4970-81a3-e4eda598333c/resourcegroups/luiz.victor.dev-rg/workspaces/tcc-experiments/datastores/workspaceblobstore/paths/LocalUpload/f4ecea294ba5aa10eadec0cdaf59ac5c/vrex_2014.csv - name: vrex_2014\nData to train asset URI: azureml://subscriptions/a3f56f48-3efb-4970-81a3-e4eda598333c/resourcegroups/luiz.victor.dev-rg/workspaces/tcc-experiments/datastores/workspaceblobstore/paths/LocalUpload/38bbb10032af817b2a7427008e191128/vrex_2014_2015_2016_2017_2018_.csv - name: vrex_2014_2015_2016_2017_2018_\nData to test asset URI: azureml://subscriptions/a3f56f48-3efb-4970-81a3-e4eda598333c/resourcegroups/luiz.victor.dev-rg/workspaces/tcc-experiments/datastores/workspaceblobstore/paths/LocalUpload/51c997e091d027cb70ab7be8e4015a5f/vrex_2019.csv - name: vrex_2019\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1714008759547
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Components"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "data_prep_component = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/preparation/data_prep.yaml\")\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "data_prep_component = ml_client.create_or_update(data_prep_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component data_prep_vrex_defaults_model with Version 2024-04-25-01-32-40-0480398 is registered\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1714008761448
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "train_gbc = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/train/train_gbc.yaml\")\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "train_gbc = ml_client.create_or_update(train_gbc)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_gbc.name} with Version {train_gbc.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component train_gbc_model with Version 2024-04-25-01-32-41-5328012 is registered\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1714008762424
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "train_nbc = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/train/train_nbc.yaml\")\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "train_nbc = ml_client.create_or_update(train_nbc)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_nbc.name} with Version {train_nbc.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component train_nbc_model with Version 2024-04-25-01-32-42-5843384 is registered\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1714008763427
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "train_rfc = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/train/train_rfc.yaml\")\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "train_rfc = ml_client.create_or_update(train_rfc)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_rfc.name} with Version {train_rfc.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component train_rfc_model with Version 2024-04-25-01-32-43-8044732 is registered\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1714008764831
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "train_component = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/train/train_svc.yaml\")\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "train_component = ml_client.create_or_update(train_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component train_svc_model with Version 2024-04-25-01-32-45-1768033 is registered\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1714008766501
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "train_component = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/train/train_xgb.yaml\")\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "train_component = ml_client.create_or_update(train_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component train_xgb_model with Version 2024-04-25-01-32-46-3821085 is registered\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1714008767700
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOTAL_TRIALS = 20\n",
        "CONCURRENT_TRIALS = 5\n",
        "TIMEOUT = 3600\n",
        "\n",
        "COMPUTE = 'serverless'\n",
        "SAMPLING_ALGORITHM = 'bayesian'\n",
        "METRIC = 'Accuracy'\n",
        "GOAL = 'Maximize'\n"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1714008767836
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "from azure.ai.ml.sweep import Choice, Uniform\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def train_gbc_pipeline(\n",
        "    data_to_train,\n",
        "    data_to_test,\n",
        "    flag_remove_null_values,\n",
        "    flag_remove_values_by_percentage,\n",
        "    percentage_to_remove_column,\n",
        "):\n",
        "\n",
        "    data_prep_job = data_prep_component(\n",
        "        data_to_train=data_to_train,\n",
        "        data_to_test=data_to_test,\n",
        "        flag_remove_null_values=flag_remove_null_values,\n",
        "        flag_remove_values_by_percentage=flag_remove_values_by_percentage,\n",
        "        percentage_to_remove_column=percentage_to_remove_column,\n",
        "    )\n",
        "\n",
        "    train_gbc_job = train_gbc(\n",
        "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        n_estimators_to_gbc=Choice(values=[50, 100, 200]),\n",
        "        learning_rate_to_gbc=Uniform(min_value=0.01, max_value=0.3),\n",
        "    )\n",
        "\n",
        "    sweep_step_to_gbc = train_gbc_job.sweep(\n",
        "        compute=COMPUTE,\n",
        "        sampling_algorithm=SAMPLING_ALGORITHM,\n",
        "        primary_metric=METRIC,\n",
        "        goal=GOAL,\n",
        "    )\n",
        "\n",
        "    sweep_step_to_gbc.set_limits(max_total_trials=2*TOTAL_TRIALS, max_concurrent_trials=CONCURRENT_TRIALS, timeout=TIMEOUT)\n"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1714008767987
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "from azure.ai.ml.sweep import Choice, Uniform\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def train_nbc_pipeline(\n",
        "    data_to_train,\n",
        "    data_to_test,\n",
        "    flag_remove_null_values,\n",
        "    flag_remove_values_by_percentage,\n",
        "    percentage_to_remove_column,\n",
        "):\n",
        "\n",
        "    data_prep_job = data_prep_component(\n",
        "        data_to_train=data_to_train,\n",
        "        data_to_test=data_to_test,\n",
        "        flag_remove_null_values=flag_remove_null_values,\n",
        "        flag_remove_values_by_percentage=flag_remove_values_by_percentage,\n",
        "        percentage_to_remove_column=percentage_to_remove_column,\n",
        "    )\n",
        "\n",
        "    train_nbc_job = train_nbc(\n",
        "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
        "    )\n",
        "\n",
        "    sweep_step_to_nbc = train_nbc_job.sweep(\n",
        "        compute=COMPUTE,\n",
        "        sampling_algorithm=SAMPLING_ALGORITHM,\n",
        "        primary_metric=METRIC,\n",
        "        goal=GOAL,\n",
        "    )\n",
        "\n",
        "    sweep_step_to_nbc.set_limits(max_total_trials=1, max_concurrent_trials=1, timeout=TIMEOUT)\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1714008768108
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "from azure.ai.ml.sweep import Choice, Uniform\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def train_rfc_pipeline(\n",
        "    data_to_train,\n",
        "    data_to_test,\n",
        "    flag_remove_null_values,\n",
        "    learning_rate_to_train,\n",
        "    flag_remove_values_by_percentage,\n",
        "    percentage_to_remove_column,\n",
        "):\n",
        "\n",
        "    data_prep_job = data_prep_component(\n",
        "        data_to_train=data_to_train,\n",
        "        data_to_test=data_to_test,\n",
        "        flag_remove_null_values=flag_remove_null_values,\n",
        "        flag_remove_values_by_percentage=flag_remove_values_by_percentage,\n",
        "        percentage_to_remove_column=percentage_to_remove_column,\n",
        "    )\n",
        "\n",
        "    train_rfc_job = train_rfc(\n",
        "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        n_estimators_to_rfc=Choice(values=[50, 100, 200]),\n",
        "    )\n",
        "\n",
        "    sweep_step_to_rfc = train_rfc_job.sweep(\n",
        "        compute=COMPUTE,\n",
        "        sampling_algorithm=SAMPLING_ALGORITHM,\n",
        "        primary_metric=METRIC,\n",
        "        goal=GOAL,\n",
        "    )\n",
        "\n",
        "    sweep_step_to_rfc.set_limits(max_total_trials=TOTAL_TRIALS, max_concurrent_trials=CONCURRENT_TRIALS, timeout=TIMEOUT)\n"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1714008768228
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "from azure.ai.ml.sweep import Choice, Uniform\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def train_svc_pipeline(\n",
        "    data_to_train,\n",
        "    data_to_test,\n",
        "    flag_remove_null_values,\n",
        "    learning_rate_to_train,\n",
        "    flag_remove_values_by_percentage,\n",
        "    percentage_to_remove_column,\n",
        "):\n",
        "\n",
        "    data_prep_job = data_prep_component(\n",
        "        data_to_train=data_to_train,\n",
        "        data_to_test=data_to_test,\n",
        "        flag_remove_null_values=flag_remove_null_values,\n",
        "        flag_remove_values_by_percentage=flag_remove_values_by_percentage,\n",
        "        percentage_to_remove_column=percentage_to_remove_column,\n",
        "    )\n",
        "\n",
        "    train_svc_job = train_svc(\n",
        "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        kernel_to_svc=Choice(values=[\"linear\", \"rbf\", \"poly\", \"sigmoid\",\"precomputed\"]),\n",
        "        gamma_to_svc=Choice(values=[\"scale\", \"auto\"]),\n",
        "    )\n",
        "\n",
        "    sweep_step_to_svc = train_svc_job.sweep(\n",
        "        compute=COMPUTE,\n",
        "        sampling_algorithm=SAMPLING_ALGORITHM,\n",
        "        primary_metric=METRIC,\n",
        "        goal=GOAL,\n",
        "    )\n",
        "\n",
        "    sweep_step_to_svc.set_limits(max_total_trials=2*TOTAL_TRIALS, max_concurrent_trials=CONCURRENT_TRIALS, timeout=TIMEOUT)\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1714008768350
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "from azure.ai.ml.sweep import Choice, Uniform\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def train_xgb_pipeline(\n",
        "    data_to_train,\n",
        "    data_to_test,\n",
        "    flag_remove_null_values,\n",
        "    learning_rate_to_train,\n",
        "    flag_remove_values_by_percentage,\n",
        "    percentage_to_remove_column,\n",
        "):\n",
        "\n",
        "    data_prep_job = data_prep_component(\n",
        "        data_to_train=data_to_train,\n",
        "        data_to_test=data_to_test,\n",
        "        flag_remove_null_values=flag_remove_null_values,\n",
        "        flag_remove_values_by_percentage=flag_remove_values_by_percentage,\n",
        "        percentage_to_remove_column=percentage_to_remove_column,\n",
        "    )\n",
        "\n",
        "    train_xgb_job = train_xgb(\n",
        "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        n_estimators_to_xgb=Choice(values=[100, 500, 1000]),\n",
        "        learning_rate_to_xgb=Uniform(min_value=0.01, max_value=0.3),\n",
        "    )\n",
        "\n",
        "    sweep_step_to_xgb = train_xgb_job.sweep(\n",
        "        compute=COMPUTE,\n",
        "        sampling_algorithm=SAMPLING_ALGORITHM,\n",
        "        primary_metric=METRIC,\n",
        "        goal=GOAL,\n",
        "    )\n",
        "\n",
        "    sweep_step_to_xgb.set_limits(max_total_trials=2*TOTAL_TRIALS, max_concurrent_trials=CONCURRENT_TRIALS, timeout=TIMEOUT)"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1714008768497
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipelines = []\n",
        "\n",
        "for data_to_train, data_to_test in zip(arr_data_to_train, arr_data_to_test):\n",
        "\n",
        "    pipeline = train_gbc_pipeline(\n",
        "        data_to_train=Input(type=\"uri_file\", path=data_to_train.path),\n",
        "        data_to_test=Input(type=\"uri_file\", path=data_to_test.path),\n",
        "        flag_remove_null_values=False,\n",
        "        flag_remove_values_by_percentage=False,\n",
        "        percentage_to_remove_column=0,\n",
        "    )\n",
        "    \n",
        "    pipelines.append(pipeline)\n",
        "\n",
        "    pipeline = train_nbc_pipeline(\n",
        "        data_to_train=Input(type=\"uri_file\", path=data_to_train.path),\n",
        "        data_to_test=Input(type=\"uri_file\", path=data_to_test.path),\n",
        "        flag_remove_null_values=False,\n",
        "        flag_remove_values_by_percentage=False,\n",
        "        percentage_to_remove_column=0,\n",
        "    )\n",
        "    \n",
        "    pipelines.append(pipeline)\n",
        "\n",
        "    pipeline = train_rfc_pipeline(\n",
        "        data_to_train=Input(type=\"uri_file\", path=data_to_train.path),\n",
        "        data_to_test=Input(type=\"uri_file\", path=data_to_test.path),\n",
        "        flag_remove_null_values=False,\n",
        "        flag_remove_values_by_percentage=False,\n",
        "        percentage_to_remove_column=0,\n",
        "    )\n",
        "    \n",
        "    pipelines.append(pipeline)\n",
        "\n",
        "    pipeline = train_svc_pipeline(\n",
        "        data_to_train=Input(type=\"uri_file\", path=data_to_train.path),\n",
        "        data_to_test=Input(type=\"uri_file\", path=data_to_test.path),\n",
        "        flag_remove_null_values=False,\n",
        "        flag_remove_values_by_percentage=False,\n",
        "        percentage_to_remove_column=0,\n",
        "    )\n",
        "    \n",
        "    pipelines.append(pipeline)\n",
        "\n",
        "    pipeline = train_xgb_pipeline(\n",
        "        data_to_train=Input(type=\"uri_file\", path=data_to_train.path),\n",
        "        data_to_test=Input(type=\"uri_file\", path=data_to_test.path),\n",
        "        flag_remove_null_values=False,\n",
        "        flag_remove_values_by_percentage=False,\n",
        "        percentage_to_remove_column=0,\n",
        "    )\n",
        "    \n",
        "    pipelines.append(pipeline)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_svc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 35\u001b[0m\n\u001b[1;32m     25\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m train_rfc_pipeline(\n\u001b[1;32m     26\u001b[0m     data_to_train\u001b[38;5;241m=\u001b[39mInput(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muri_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, path\u001b[38;5;241m=\u001b[39mdata_to_train\u001b[38;5;241m.\u001b[39mpath),\n\u001b[1;32m     27\u001b[0m     data_to_test\u001b[38;5;241m=\u001b[39mInput(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muri_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, path\u001b[38;5;241m=\u001b[39mdata_to_test\u001b[38;5;241m.\u001b[39mpath),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     percentage_to_remove_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m pipelines\u001b[38;5;241m.\u001b[39mappend(pipeline)\n\u001b[0;32m---> 35\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_svc_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_to_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muri_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_to_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_to_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muri_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_to_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflag_remove_null_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflag_remove_values_by_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpercentage_to_remove_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m pipelines\u001b[38;5;241m.\u001b[39mappend(pipeline)\n\u001b[1;32m     45\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m train_xgb_pipeline(\n\u001b[1;32m     46\u001b[0m     data_to_train\u001b[38;5;241m=\u001b[39mInput(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muri_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, path\u001b[38;5;241m=\u001b[39mdata_to_train\u001b[38;5;241m.\u001b[39mpath),\n\u001b[1;32m     47\u001b[0m     data_to_test\u001b[38;5;241m=\u001b[39mInput(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muri_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, path\u001b[38;5;241m=\u001b[39mdata_to_test\u001b[38;5;241m.\u001b[39mpath),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     percentage_to_remove_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     51\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/dsl/_pipeline_decorator.py:180\u001b[0m, in \u001b[0;36mpipeline.<locals>.pipeline_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     non_pipeline_params_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    176\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m provided_positional_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m non_pipeline_inputs\n\u001b[1;32m    177\u001b[0m     }\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO: cache built pipeline component\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     pipeline_component \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_provided_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovided_positional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_pipeline_inputs_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_pipeline_params_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_pipeline_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_pipeline_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# use `finally` to ensure pop operation from the stack\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     dsl_settings \u001b[38;5;241m=\u001b[39m _dsl_settings_stack\u001b[38;5;241m.\u001b[39mpop()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/dsl/_pipeline_component_builder.py:180\u001b[0m, in \u001b[0;36mPipelineComponentBuilder.build\u001b[0;34m(self, user_provided_kwargs, non_pipeline_inputs_dict, non_pipeline_inputs)\u001b[0m\n\u001b[1;32m    177\u001b[0m _definition_builder_stack\u001b[38;5;241m.\u001b[39mpush(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     outputs, _locals \u001b[38;5;241m=\u001b[39m \u001b[43mget_outputs_and_locals\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     _definition_builder_stack\u001b[38;5;241m.\u001b[39mpop()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_utils/_func_utils.py:407\u001b[0m, in \u001b[0;36mget_outputs_and_locals\u001b[0;34m(func, _all_kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_outputs_and_locals\u001b[39m(func, _all_kwargs):\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get outputs and locals from self.func. Locals will be used to update node variable names.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    :param func: The function to execute.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m    :rtype: typing.Tuple[typing.Dict, typing.Dict]\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_persistent_locals_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_all_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_utils/_func_utils.py:57\u001b[0m, in \u001b[0;36mPersistentLocalsFunctionBuilder.call\u001b[0;34m(self, func, _all_kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minjected_param \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\u001b[38;5;241m.\u001b[39mco_varnames:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconflict_argument\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\u001b[38;5;241m.\u001b[39mco_varnames)))\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_all_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_utils/_func_utils.py:93\u001b[0m, in \u001b[0;36mPersistentLocalsFunctionProfilerBuilder._call\u001b[0;34m(self, func, _all_kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m func_variable_profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_func_variable_tracer(_locals, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_sys_profiler(func_variable_profiler):\n\u001b[0;32m---> 93\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_all_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, _locals\n",
            "Cell \u001b[0;32mIn[14], line 26\u001b[0m, in \u001b[0;36mtrain_svc_pipeline\u001b[0;34m(data_to_train, data_to_test, flag_remove_null_values, learning_rate_to_train, flag_remove_values_by_percentage, percentage_to_remove_column)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;129m@dsl\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m      6\u001b[0m     compute\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserverless\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE2E data_perp-train pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     percentage_to_remove_column,\n\u001b[1;32m     16\u001b[0m ):\n\u001b[1;32m     18\u001b[0m     data_prep_job \u001b[38;5;241m=\u001b[39m data_prep_component(\n\u001b[1;32m     19\u001b[0m         data_to_train\u001b[38;5;241m=\u001b[39mdata_to_train,\n\u001b[1;32m     20\u001b[0m         data_to_test\u001b[38;5;241m=\u001b[39mdata_to_test,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m         percentage_to_remove_column\u001b[38;5;241m=\u001b[39mpercentage_to_remove_column,\n\u001b[1;32m     24\u001b[0m     )\n\u001b[0;32m---> 26\u001b[0m     train_svc_job \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_svc\u001b[49m(\n\u001b[1;32m     27\u001b[0m         train_data\u001b[38;5;241m=\u001b[39mdata_prep_job\u001b[38;5;241m.\u001b[39moutputs\u001b[38;5;241m.\u001b[39mtrain_data,  \u001b[38;5;66;03m# note: using outputs from previous step\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         test_data\u001b[38;5;241m=\u001b[39mdata_prep_job\u001b[38;5;241m.\u001b[39moutputs\u001b[38;5;241m.\u001b[39mtest_data,  \u001b[38;5;66;03m# note: using outputs from previous step\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         kernel_to_svc\u001b[38;5;241m=\u001b[39mChoice(values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoly\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     30\u001b[0m         gamma_to_svc\u001b[38;5;241m=\u001b[39mChoice(values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     33\u001b[0m     sweep_step_to_svc \u001b[38;5;241m=\u001b[39m train_svc_job\u001b[38;5;241m.\u001b[39msweep(\n\u001b[1;32m     34\u001b[0m         compute\u001b[38;5;241m=\u001b[39mCOMPUTE,\n\u001b[1;32m     35\u001b[0m         sampling_algorithm\u001b[38;5;241m=\u001b[39mSAMPLING_ALGORITHM,\n\u001b[1;32m     36\u001b[0m         primary_metric\u001b[38;5;241m=\u001b[39mMETRIC,\n\u001b[1;32m     37\u001b[0m         goal\u001b[38;5;241m=\u001b[39mGOAL,\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     40\u001b[0m     sweep_step_to_svc\u001b[38;5;241m.\u001b[39mset_limits(max_total_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mTOTAL_TRIALS, max_concurrent_trials\u001b[38;5;241m=\u001b[39mCONCURRENT_TRIALS, timeout\u001b[38;5;241m=\u001b[39mTIMEOUT)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_svc' is not defined"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714008768684
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "\n",
        "models = ['gbc', 'nbc', 'rfc', 'svc', 'xgb']\n",
        "\n",
        "def _get_experiment_names() -> [str]:\n",
        "    experiment_names = []\n",
        "    for train_name, test_name in zip(TRAIN_DATAS, TEST_DATAS):\n",
        "        for model_name in models:\n",
        "            current_time = dt.datetime.now()\n",
        "            formatted_time = current_time.strftime(\"%Y_%m_%d_%H_%M_%S\")  # Formata a data e hora atual\n",
        "            train_name_base = train_name.split('.')[0]\n",
        "            test_name_base = test_name.split('.')[0]\n",
        "            name = f\"{train_name_base}tested_{test_name_base}_model_{model_name}\"\n",
        "            experiment_names.append(name)\n",
        "            print(name)\n",
        "    return experiment_names\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714008768796
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_names = _get_experiment_names()\n",
        "for pipeline, experiment_name in zip(pipelines, experiment_names):\n",
        "    pipeline_job = ml_client.jobs.create_or_update(\n",
        "        pipeline,\n",
        "        experiment_name=experiment_name,\n",
        "    )\n",
        "\n",
        "    ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714008768813
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}