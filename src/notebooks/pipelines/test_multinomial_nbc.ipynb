{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1716433429163
        }
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'MULT_GBC_PATH' from 'pipeline_utils' (/mnt/batch/tasks/shared/LS_root/mounts/clusters/machine-to-pipelines/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/notebooks/pipelines/pipeline_utils.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLClient\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpipeline_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     COMPUTE, CONCURRENT_TRIALS, DELAY_EVALUATION, EVALUATION_INTERVAL, GOAL, METRIC, SAMPLING_ALGORITHM, TIMEOUT, TIMEOUT_PLUS, TOTAL_TRIALS, \u001b[38;5;66;03m#SWEEP\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     SUBSCRIPTION, RESOURCE_GROUP, WS_NAME,  \u001b[38;5;66;03m#AUTHENTICATE\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     MULT_GBC_PATH,    \u001b[38;5;66;03m#COMPONENTS PATHS\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     GINI_PATH,    \u001b[38;5;66;03m#FEAT SEL PATHS\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     PREP_DATA_PATH,  \u001b[38;5;66;03m#PREP DATA PATH\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     MULT_NBC_BY_GINI, NBC_BY_GINI, RFC_BY_GINI, SVC_BY_GINI, XGB_BY_GINI,    \u001b[38;5;66;03m#PIPELINES\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     GINI,   \u001b[38;5;66;03m#FEAT SEL\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     get_experiment_names,   \u001b[38;5;66;03m#FUNCTIONS\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# authenticate\u001b[39;00m\n\u001b[1;32m     15\u001b[0m credential \u001b[38;5;241m=\u001b[39m DefaultAzureCredential()\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'MULT_GBC_PATH' from 'pipeline_utils' (/mnt/batch/tasks/shared/LS_root/mounts/clusters/machine-to-pipelines/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/notebooks/pipelines/pipeline_utils.py)"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from pipeline_utils import (\n",
        "    COMPUTE, CONCURRENT_TRIALS, DELAY_EVALUATION, EVALUATION_INTERVAL, GOAL, METRIC, SAMPLING_ALGORITHM, TIMEOUT, TIMEOUT_PLUS, TOTAL_TRIALS, #SWEEP\n",
        "    SUBSCRIPTION, RESOURCE_GROUP, WS_NAME,  #AUTHENTICATE\n",
        "    MULT_GBC_PATH,    #COMPONENTS PATHS\n",
        "    GINI_PATH,    #FEAT SEL PATHS\n",
        "    PREP_DATA_PATH,  #PREP DATA PATH\n",
        "    MULT_NBC_BY_GINI, NBC_BY_GINI, RFC_BY_GINI, SVC_BY_GINI, XGB_BY_GINI,    #PIPELINES\n",
        "    GINI,   #FEAT SEL\n",
        "    get_experiment_names,   #FUNCTIONS\n",
        ")\n",
        "\n",
        "# authenticate\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=SUBSCRIPTION,\n",
        "    resource_group_name=RESOURCE_GROUP,\n",
        "    workspace_name=WS_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716433429673
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "TRAIN_DATAS = [\n",
        "    \"vrex_encoded_tf_idf_2008_2009_2010_2011_2012_2013_2014_2015_2016_2017.csv\"\n",
        "]\n",
        "\n",
        "TEST_DATAS = [\n",
        "    \"vrex_encoded_tf_idf_2018_2019_2020_2021.csv\"\n",
        "]\n",
        "\n",
        "version = \"v1\"\n",
        "\n",
        "arr_data_to_train = []\n",
        "arr_data_to_test = []\n",
        "\n",
        "for to_train, to_test in zip(TRAIN_DATAS, TEST_DATAS):\n",
        "    data_to_train = ml_client.data.get(name=to_train.split(\".\")[0], version=version)\n",
        "    arr_data_to_train.append(data_to_train)\n",
        "    print(f\"Data to train asset URI: {data_to_train.path} - name: {to_train.split('.')[0]}\")\n",
        "\n",
        "    data_to_test = ml_client.data.get(name=to_test.split('.')[0], version=version)\n",
        "    arr_data_to_test.append(data_to_test)\n",
        "    print(f\"Data to test asset URI: {data_to_test.path} - name: {to_test.split('.')[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "data_prep_component = load_component(source=PREP_DATA_PATH)\n",
        "data_prep_component = ml_client.create_or_update(data_prep_component)\n",
        "\n",
        "feat_sel_component = load_component(source=GINI_PATH)\n",
        "feat_sel_component = ml_client.create_or_update(feat_sel_component)\n",
        "\n",
        "mult_nbc_train_gbc = load_component(source=MULT_GBC_PATH)\n",
        "mult_nbc_train_gbc = ml_client.create_or_update(mult_nbc_train_gbc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716433440450
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "from azure.ai.ml.sweep import Choice, Uniform, MedianStoppingPolicy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716433440554
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    name=MULT_NBC_BY_GINI,\n",
        "    compute=COMPUTE,\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def train_mult_nbc_pipeline(\n",
        "    data_to_train,\n",
        "    data_to_test,\n",
        "    feature_quantity,\n",
        "    flag_remove_null_values,\n",
        "    flag_remove_values_by_percentage,\n",
        "    percentage_to_remove_column,\n",
        "):\n",
        "\n",
        "    data_prep_job = data_prep_component(\n",
        "        data_to_train=data_to_train,\n",
        "        data_to_test=data_to_test,\n",
        "        flag_remove_null_values=flag_remove_null_values,\n",
        "        flag_remove_values_by_percentage=flag_remove_values_by_percentage,\n",
        "        percentage_to_remove_column=percentage_to_remove_column,\n",
        "    )\n",
        "\n",
        "    feat_sel_job = feat_sel_component(\n",
        "        train_data=data_prep_job.outputs.train_data,\n",
        "        test_data=data_prep_job.outputs.test_data,\n",
        "        feature_quantity=feature_quantity,\n",
        "    )\n",
        "\n",
        "    mult_nbc_train_job = mult_nbc_train_gbc(\n",
        "        train_data=feat_sel_job.outputs.train_data_feat_sel,  \n",
        "        test_data=feat_sel_job.outputs.test_data_feat_sel,  \n",
        "        alpha=Choice(values=[0.01, 0.1, 0.5, 1.0, 2, 5]),\n",
        "        fit_prior=Choice(values=[True, False]),\n",
        "    )\n",
        "\n",
        "    sweep_step_to_mult_nbc = mult_nbc_train_job.sweep(\n",
        "        compute=COMPUTE,\n",
        "        sampling_algorithm=SAMPLING_ALGORITHM,\n",
        "        primary_metric=METRIC,\n",
        "        goal=GOAL,\n",
        "    )\n",
        "\n",
        "    sweep_step_to_mult_nbc.set_limits(max_total_trials=2*TOTAL_TRIALS, max_concurrent_trials=CONCURRENT_TRIALS, timeout=TIMEOUT_PLUS)\n",
        "\n",
        "    sweep_step_to_mult_nbc.early_termination = MedianStoppingPolicy(delay_evaluation=DELAY_EVALUATION, evaluation_interval=EVALUATION_INTERVAL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716433441082
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pipelines = []\n",
        "n_features = [399, 200, 100, 50]\n",
        "\n",
        "for n_feature in n_features:\n",
        "    for data_to_train, data_to_test in zip(arr_data_to_train, arr_data_to_test):\n",
        "\n",
        "        pipelines.append(train_mult_nbc_pipeline(\n",
        "            data_to_train=Input(type=\"uri_file\", path=data_to_train.path),\n",
        "            data_to_test=Input(type=\"uri_file\", path=data_to_test.path),\n",
        "            feature_quantity=n_feature,\n",
        "            flag_remove_null_values=False,\n",
        "            flag_remove_values_by_percentage=False,\n",
        "            percentage_to_remove_column=0,\n",
        "        ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716471286642
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "experiment_names = get_experiment_names(TRAIN_DATAS, TEST_DATAS, GINI, n_features)\n",
        "for pipeline, experiment_name in zip(pipelines, experiment_names):\n",
        "    pipeline_job = ml_client.jobs.create_or_update(\n",
        "        pipeline,\n",
        "        experiment_name=experiment_name,\n",
        "    )\n",
        "    ml_client.jobs.stream(pipeline_job.name)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "azureml_py310_sdkv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
