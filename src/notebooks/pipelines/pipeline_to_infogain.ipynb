{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1716433429163
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from notebooks_utils import COMPUTE, CONCURRENT_TRIALS, DELAY_EVALUATION, EVALUATION_INTERVAL, GOAL, METRIC, SAMPLING_ALGORITHM, TIMEOUT, TIMEOUT_PLUS, TOTAL_TRIALS\n",
        "\n",
        "# authenticate\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "SUBSCRIPTION=\"a3f56f48-3efb-4970-81a3-e4eda598333c\"\n",
        "RESOURCE_GROUP=\"luiz.victor.dev-rg\"\n",
        "WS_NAME=\"tcc-experiments\"\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=SUBSCRIPTION,\n",
        "    resource_group_name=RESOURCE_GROUP,\n",
        "    workspace_name=WS_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1716433429673
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data to train asset URI: azureml://subscriptions/a3f56f48-3efb-4970-81a3-e4eda598333c/resourcegroups/luiz.victor.dev-rg/workspaces/tcc-experiments/datastores/workspaceblobstore/paths/LocalUpload/3cfdebd258ea2db7363c55bb841ca887/vrex_encoded_tf_idf_2008_2009_2010_2011_2012_2013_2014_2015_2016_2017.csv - name: vrex_encoded_tf_idf_2008_2009_2010_2011_2012_2013_2014_2015_2016_2017\n",
            "Data to test asset URI: azureml://subscriptions/a3f56f48-3efb-4970-81a3-e4eda598333c/resourcegroups/luiz.victor.dev-rg/workspaces/tcc-experiments/datastores/workspaceblobstore/paths/LocalUpload/3992df39ce41e34367e629d6656f4e9f/vrex_encoded_tf_idf_2018_2019_2020_2021.csv - name: vrex_encoded_tf_idf_2018_2019_2020_2021\n"
          ]
        }
      ],
      "source": [
        "TRAIN_DATAS = [\n",
        "    \"vrex_encoded_tf_idf_2008_2009_2010_2011_2012_2013_2014_2015_2016_2017.csv\"\n",
        "]\n",
        "\n",
        "TEST_DATAS = [\n",
        "    \"vrex_encoded_tf_idf_2018_2019_2020_2021.csv\"\n",
        "]\n",
        "\n",
        "version = \"v1\"\n",
        "\n",
        "arr_data_to_train = []\n",
        "arr_data_to_test = []\n",
        "\n",
        "for to_train, to_test in zip(TRAIN_DATAS, TEST_DATAS):\n",
        "    data_to_train = ml_client.data.get(name=to_train.split(\".\")[0], version=version)\n",
        "    arr_data_to_train.append(data_to_train)\n",
        "    print(f\"Data to train asset URI: {data_to_train.path} - name: {to_train.split('.')[0]}\")\n",
        "\n",
        "    data_to_test = ml_client.data.get(name=to_test.split('.')[0], version=version)\n",
        "    arr_data_to_test.append(data_to_test)\n",
        "    print(f\"Data to test asset URI: {data_to_test.path} - name: {to_test.split('.')[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading feature_selection (0.02 MBs): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16915/16915 [00:00<00:00, 73332.15it/s]\u001b[0m\n",
            "\u001b[39m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "data_prep_component = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/preparation/data_prep.yaml\")\n",
        "data_prep_component = ml_client.create_or_update(data_prep_component)\n",
        "\n",
        "infogain_component = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/feature_selection/infogain.yaml\")\n",
        "infogain_component = ml_client.create_or_update(infogain_component)\n",
        "\n",
        "train_gbc = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/train/train_gbc.yaml\")\n",
        "train_gbc = ml_client.create_or_update(train_gbc)\n",
        "\n",
        "train_nbc = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/train/train_nbc.yaml\")\n",
        "train_nbc = ml_client.create_or_update(train_nbc)\n",
        "\n",
        "train_rfc = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/train/train_rfc.yaml\")\n",
        "train_rfc = ml_client.create_or_update(train_rfc)\n",
        "\n",
        "train_svc = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/train/train_svc.yaml\")\n",
        "train_svc = ml_client.create_or_update(train_svc)\n",
        "\n",
        "train_xgb = load_component(source=\"/home/azureuser/cloudfiles/code/Users/luiz.victor.dev/tcc_experiments_az_ml/src/components/train/train_xgb.yaml\")\n",
        "train_xgb = ml_client.create_or_update(train_xgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1716433440450
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "from azure.ai.ml.sweep import Choice, Uniform, MedianStoppingPolicy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1716433440554
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def train_gbc_pipeline(\n",
        "    data_to_train,\n",
        "    data_to_test,\n",
        "    flag_remove_null_values,\n",
        "    flag_remove_values_by_percentage,\n",
        "    percentage_to_remove_column,\n",
        "):\n",
        "\n",
        "    data_prep_job = data_prep_component(\n",
        "        data_to_train=data_to_train,\n",
        "        data_to_test=data_to_test,\n",
        "        flag_remove_null_values=flag_remove_null_values,\n",
        "        flag_remove_values_by_percentage=flag_remove_values_by_percentage,\n",
        "        percentage_to_remove_column=percentage_to_remove_column,\n",
        "    )\n",
        "\n",
        "    infogain_job = infogain_component(\n",
        "        train_data=data_prep_job.outputs.train_data,\n",
        "        test_data=data_prep_job.outputs.test_data,\n",
        "        feature_percentage=0.5,\n",
        "    )\n",
        "\n",
        "    train_gbc_job = train_gbc(\n",
        "        train_data=infogain_job.outputs.train_data_feat_sel,  \n",
        "        test_data=infogain_job.outputs.test_data_feat_sel,  \n",
        "        n_estimators_to_gbc=Choice(values=[50, 100, 200]),\n",
        "        learning_rate_to_gbc=Uniform(min_value=0.01, max_value=0.3),\n",
        "    )\n",
        "\n",
        "    sweep_step_to_gbc = train_gbc_job.sweep(\n",
        "        compute=COMPUTE,\n",
        "        sampling_algorithm=SAMPLING_ALGORITHM,\n",
        "        primary_metric=METRIC,\n",
        "        goal=GOAL,\n",
        "    )\n",
        "\n",
        "    sweep_step_to_gbc.set_limits(max_total_trials=2*TOTAL_TRIALS, max_concurrent_trials=CONCURRENT_TRIALS, timeout=TIMEOUT)\n",
        "\n",
        "    sweep_step_to_gbc.early_termination = MedianStoppingPolicy(delay_evaluation=DELAY_EVALUATION, evaluation_interval=EVALUATION_INTERVAL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def train_nbc_pipeline(\n",
        "    data_to_train,\n",
        "    data_to_test,\n",
        "    flag_remove_null_values,\n",
        "    flag_remove_values_by_percentage,\n",
        "    percentage_to_remove_column,\n",
        "):\n",
        "\n",
        "    data_prep_job = data_prep_component(\n",
        "        data_to_train=data_to_train,\n",
        "        data_to_test=data_to_test,\n",
        "        flag_remove_null_values=flag_remove_null_values,\n",
        "        flag_remove_values_by_percentage=flag_remove_values_by_percentage,\n",
        "        percentage_to_remove_column=percentage_to_remove_column,\n",
        "    )\n",
        "\n",
        "    infogain_job = infogain_component(\n",
        "        train_data=data_prep_job.outputs.train_data,\n",
        "        test_data=data_prep_job.outputs.test_data,\n",
        "        feature_percentage=0.5,\n",
        "    )\n",
        "\n",
        "    train_nbc_job = train_nbc(\n",
        "        train_data=infogain_job.outputs.train_data_feat_sel,  \n",
        "        test_data=infogain_job.outputs.test_data_feat_sel,  \n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "gather": {
          "logged": 1716433440739
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def train_rfc_pipeline(\n",
        "    data_to_train,\n",
        "    data_to_test,\n",
        "    flag_remove_null_values,\n",
        "    flag_remove_values_by_percentage,\n",
        "    percentage_to_remove_column,\n",
        "):\n",
        "\n",
        "    data_prep_job = data_prep_component(\n",
        "        data_to_train=data_to_train,\n",
        "        data_to_test=data_to_test,\n",
        "        flag_remove_null_values=flag_remove_null_values,\n",
        "        flag_remove_values_by_percentage=flag_remove_values_by_percentage,\n",
        "        percentage_to_remove_column=percentage_to_remove_column,\n",
        "    )\n",
        "\n",
        "    infogain_job = infogain_component(\n",
        "        train_data=data_prep_job.outputs.train_data,\n",
        "        test_data=data_prep_job.outputs.test_data,\n",
        "        feature_percentage=0.5,\n",
        "    )\n",
        "\n",
        "    train_rfc_job = train_rfc(\n",
        "        train_data=infogain_job.outputs.train_data_feat_sel,  \n",
        "        test_data=infogain_job.outputs.test_data_feat_sel, \n",
        "        n_estimators_to_rfc=Choice(values=[50, 100, 200]),\n",
        "    )\n",
        "\n",
        "    sweep_step_to_rfc = train_rfc_job.sweep(\n",
        "        compute=COMPUTE,\n",
        "        sampling_algorithm=SAMPLING_ALGORITHM,\n",
        "        primary_metric=METRIC,\n",
        "        goal=GOAL,\n",
        "    )\n",
        "\n",
        "    sweep_step_to_rfc.set_limits(max_total_trials=TOTAL_TRIALS, max_concurrent_trials=CONCURRENT_TRIALS, timeout=TIMEOUT)\n",
        "    sweep_step_to_rfc.early_termination = MedianStoppingPolicy(delay_evaluation=DELAY_EVALUATION, evaluation_interval=EVALUATION_INTERVAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "gather": {
          "logged": 1716433440831
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def train_svc_pipeline(\n",
        "    data_to_train,\n",
        "    data_to_test,\n",
        "    flag_remove_null_values,\n",
        "    flag_remove_values_by_percentage,\n",
        "    percentage_to_remove_column,\n",
        "):\n",
        "\n",
        "    data_prep_job = data_prep_component(\n",
        "        data_to_train=data_to_train,\n",
        "        data_to_test=data_to_test,\n",
        "        flag_remove_null_values=flag_remove_null_values,\n",
        "        flag_remove_values_by_percentage=flag_remove_values_by_percentage,\n",
        "        percentage_to_remove_column=percentage_to_remove_column,\n",
        "    )\n",
        "\n",
        "    infogain_job = infogain_component(\n",
        "        train_data=data_prep_job.outputs.train_data,\n",
        "        test_data=data_prep_job.outputs.test_data,\n",
        "        feature_percentage=0.5,\n",
        "    )\n",
        "\n",
        "    train_svc_job = train_svc(\n",
        "        train_data=infogain_job.outputs.train_data_feat_sel,  \n",
        "        test_data=infogain_job.outputs.test_data_feat_sel,   \n",
        "        kernel_to_svc=Choice(values=[\"linear\", \"rbf\", \"poly\", \"sigmoid\",\"precomputed\"]),\n",
        "        gamma_to_svc=Choice(values=[\"scale\", \"auto\"]),\n",
        "    )\n",
        "\n",
        "    sweep_step_to_svc = train_svc_job.sweep(\n",
        "        compute=COMPUTE,\n",
        "        sampling_algorithm=SAMPLING_ALGORITHM,\n",
        "        primary_metric=METRIC,\n",
        "        goal=GOAL,\n",
        "    )\n",
        "\n",
        "    sweep_step_to_svc.set_limits(max_total_trials=2*TOTAL_TRIALS, max_concurrent_trials=CONCURRENT_TRIALS, timeout=TIMEOUT_PLUS)\n",
        "    sweep_step_to_svc.early_termination = MedianStoppingPolicy(delay_evaluation=DELAY_EVALUATION, evaluation_interval=EVALUATION_INTERVAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1716433440916
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def train_xgb_pipeline(\n",
        "    data_to_train,\n",
        "    data_to_test,\n",
        "    flag_remove_null_values,\n",
        "    flag_remove_values_by_percentage,\n",
        "    percentage_to_remove_column,\n",
        "):\n",
        "\n",
        "    data_prep_job = data_prep_component(\n",
        "        data_to_train=data_to_train,\n",
        "        data_to_test=data_to_test,\n",
        "        flag_remove_null_values=flag_remove_null_values,\n",
        "        flag_remove_values_by_percentage=flag_remove_values_by_percentage,\n",
        "        percentage_to_remove_column=percentage_to_remove_column,\n",
        "    )\n",
        "\n",
        "    infogain_job = infogain_component(\n",
        "        train_data=data_prep_job.outputs.train_data,\n",
        "        test_data=data_prep_job.outputs.test_data,\n",
        "        feature_percentage=0.5,\n",
        "    )\n",
        "\n",
        "    train_xgb_job = train_xgb(\n",
        "        train_data=infogain_job.outputs.train_data_feat_sel,  \n",
        "        test_data=infogain_job.outputs.test_data_feat_sel,   \n",
        "        n_estimators_to_xgb=Choice(values=[100, 500, 1000]),\n",
        "        learning_rate_to_xgb=Uniform(min_value=0.01, max_value=0.3),\n",
        "    )\n",
        "\n",
        "    sweep_step_to_xgb = train_xgb_job.sweep(\n",
        "        compute=COMPUTE,\n",
        "        sampling_algorithm=SAMPLING_ALGORITHM,\n",
        "        primary_metric=METRIC,\n",
        "        goal=GOAL,\n",
        "    )\n",
        "\n",
        "    sweep_step_to_xgb.set_limits(max_total_trials=2*TOTAL_TRIALS, max_concurrent_trials=CONCURRENT_TRIALS, timeout=TIMEOUT_PLUS)\n",
        "    sweep_step_to_xgb.early_termination = MedianStoppingPolicy(delay_evaluation=DELAY_EVALUATION, evaluation_interval=EVALUATION_INTERVAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "gather": {
          "logged": 1716433441082
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pipelines = []\n",
        "\n",
        "for data_to_train, data_to_test in zip(arr_data_to_train, arr_data_to_test):\n",
        "\n",
        "    pipelines.append(train_nbc_pipeline(\n",
        "        data_to_train=Input(type=\"uri_file\", path=data_to_train.path),\n",
        "        data_to_test=Input(type=\"uri_file\", path=data_to_test.path),\n",
        "        flag_remove_null_values=False,\n",
        "        flag_remove_values_by_percentage=False,\n",
        "        percentage_to_remove_column=0,\n",
        "    ))\n",
        "\n",
        "    pipelines.append(train_gbc_pipeline(\n",
        "        data_to_train=Input(type=\"uri_file\", path=data_to_train.path),\n",
        "        data_to_test=Input(type=\"uri_file\", path=data_to_test.path),\n",
        "        flag_remove_null_values=False,\n",
        "        flag_remove_values_by_percentage=False,\n",
        "        percentage_to_remove_column=0,\n",
        "    ))\n",
        "\n",
        "    pipelines.append(train_rfc_pipeline(\n",
        "        data_to_train=Input(type=\"uri_file\", path=data_to_train.path),\n",
        "        data_to_test=Input(type=\"uri_file\", path=data_to_test.path),\n",
        "        flag_remove_null_values=False,\n",
        "        flag_remove_values_by_percentage=False,\n",
        "        percentage_to_remove_column=0,\n",
        "    ))\n",
        "\n",
        "    pipelines.append(train_svc_pipeline(\n",
        "        data_to_train=Input(type=\"uri_file\", path=data_to_train.path),\n",
        "        data_to_test=Input(type=\"uri_file\", path=data_to_test.path),\n",
        "        flag_remove_null_values=False,\n",
        "        flag_remove_values_by_percentage=False,\n",
        "        percentage_to_remove_column=0,\n",
        "    ))\n",
        "\n",
        "    pipelines.append(train_xgb_pipeline(\n",
        "        data_to_train=Input(type=\"uri_file\", path=data_to_train.path),\n",
        "        data_to_test=Input(type=\"uri_file\", path=data_to_test.path),\n",
        "        flag_remove_null_values=False,\n",
        "        flag_remove_values_by_percentage=False,\n",
        "        percentage_to_remove_column=0,\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "gather": {
          "logged": 1716433441170
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "\n",
        "models = ['nbc', 'gbc', 'rfc', 'svc', 'xgb']\n",
        "\n",
        "def _get_experiment_names() -> [str]:\n",
        "    experiment_names = []\n",
        "    for train_name, test_name in zip(TRAIN_DATAS, TEST_DATAS):\n",
        "        for model_name in models:\n",
        "            current_time = dt.datetime.now()\n",
        "            formatted_time = current_time.strftime(\"%Y_%m_%d_%H_%M_%S\")  # Formata a data e hora atual\n",
        "            train_name_base = train_name.split('.')[0]\n",
        "            test_name_base = test_name.split('.')[0]\n",
        "            name = f\"{train_name_base}_tested_{test_name_base}_model_{model_name}_encoded_tfidf_data\"\n",
        "            experiment_names.append(name)\n",
        "            print(name)\n",
        "    return experiment_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1716471286642
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vrex_encoded_tf_idf_2008_2009_2010_2011_2012_2013_2014_2015_2016_2017_tested_vrex_encoded_tf_idf_2018_2019_2020_2021_model_nbc_encoded_tfidf_data\n",
            "vrex_encoded_tf_idf_2008_2009_2010_2011_2012_2013_2014_2015_2016_2017_tested_vrex_encoded_tf_idf_2018_2019_2020_2021_model_gbc_encoded_tfidf_data\n",
            "vrex_encoded_tf_idf_2008_2009_2010_2011_2012_2013_2014_2015_2016_2017_tested_vrex_encoded_tf_idf_2018_2019_2020_2021_model_rfc_encoded_tfidf_data\n",
            "vrex_encoded_tf_idf_2008_2009_2010_2011_2012_2013_2014_2015_2016_2017_tested_vrex_encoded_tf_idf_2018_2019_2020_2021_model_svc_encoded_tfidf_data\n",
            "vrex_encoded_tf_idf_2008_2009_2010_2011_2012_2013_2014_2015_2016_2017_tested_vrex_encoded_tf_idf_2018_2019_2020_2021_model_xgb_encoded_tfidf_data\n",
            "RunId: musing_pear_jv1mng8brf\n",
            "Web View: https://ml.azure.com/runs/musing_pear_jv1mng8brf?wsid=/subscriptions/a3f56f48-3efb-4970-81a3-e4eda598333c/resourcegroups/luiz.victor.dev-rg/workspaces/tcc-experiments\n",
            "\n",
            "Streaming logs/azureml/executionlogs.txt\n",
            "========================================\n",
            "\n",
            "[2024-06-03 02:28:09Z] Submitting 1 runs, first five are: 83a23745:97df0886-658a-487b-a367-0df8c96ae3d4\n",
            "[2024-06-03 02:28:10Z] Completing processing run id 97df0886-658a-487b-a367-0df8c96ae3d4.\n",
            "[2024-06-03 02:28:11Z] Submitting 1 runs, first five are: eec72292:b55012d0-9bd4-4cd0-b355-6ec80d35dc7d\n",
            "[2024-06-03 02:36:38Z] Completing processing run id b55012d0-9bd4-4cd0-b355-6ec80d35dc7d.\n",
            "[2024-06-03 02:36:39Z] Submitting 1 runs, first five are: 16fc2961:115c04f6-7695-4883-96e9-e2c1b427fa1d\n"
          ]
        }
      ],
      "source": [
        "experiment_names = _get_experiment_names()\n",
        "for pipeline, experiment_name in zip(pipelines, experiment_names):\n",
        "    pipeline_job = ml_client.jobs.create_or_update(\n",
        "        pipeline,\n",
        "        experiment_name=experiment_name,\n",
        "    )\n",
        "    ml_client.jobs.stream(pipeline_job.name)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "azureml_py310_sdkv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
